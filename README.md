# Language Model Implementations

This repository contains from-scratch PyTorch implementations of key components in modern language models, including the attention mechanism and a fully architected GPT model.

## Project Structure

```
.
├── Attention
│   └── Attention.ipynb
├── GPT
│   ├── GPT.ipynb
│   └── input.txt
└── display_contents.sh
```

## Contents

1. **Attention Mechanism**
   - Location: `Attention/Attention.ipynb`
   - Description: This Jupyter notebook contains a detailed implementation of the attention mechanism used in language models.

2. **GPT Model**
   - Location: `GPT/GPT.ipynb`
   - Description: This Jupyter notebook provides a complete implementation of a GPT (Generative Pre-trained Transformer) model architecture.
   - Input: `GPT/input.txt` - This file likely contains sample text for testing or training the GPT model.

3. **Utility Script**
   - `display_contents.sh`: A shell script to display the contents of the project directory.

## Getting Started

1. Ensure you have PyTorch installed in your Python environment.
2. Clone this repository to your local machine.
3. Navigate to the project directory.
4. Open the Jupyter notebooks in the `Attention` and `GPT` folders to explore the implementations.

## Usage

- To understand the attention mechanism, start with `Attention/Attention.ipynb`.
- To explore the full GPT model, open `GPT/GPT.ipynb`.
- You can use the provided `input.txt` file in the GPT folder for testing the model, or replace it with your own text data.

## Contributing

Contributions to improve the implementations or add new features are welcome. Please feel free to submit pull requests or open issues for any bugs or enhancements.


